{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM, GRU 이론\n",
    "https://velog.io/@cha-suyeon/DL-Long-Short-Term-MemoryLSTM-and-GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# windowed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. expand_dim()\n",
    "\n",
    "tensorflow의 expand_dim은 차원을 늘려준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([20, 1])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(20)\n",
    "print(x.shape)\n",
    "\n",
    "tf.expand_dims(x, 1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. batch 메서드 \n",
    "\n",
    "모델에 학습시킬 때 batch_size를 지정하여 batch_size만큼 데이터를 읽어들여 학습시킬 때 유용한 method이다. 이미지와 같은 큰 사이즈는 memory에 한 번에 올라가지 못하기 때문에 batch를 나누어 학습시키기도 한다. 또한 모델이 weight를 업데이트 할 때, 1개의 batch가 끝나고 난 후 업데이트를 하게 되는데, 업데이트 빈도를 조절하는 효과도 있다. \n",
    "\n",
    "옵션\n",
    "\n",
    "+ drop_remainder : 마지막 남은 데이터를 drop할 것인지 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2], shape=(3,), dtype=int64)\n",
      "tf.Tensor([3 4 5], shape=(3,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.range(8)\n",
    "\n",
    "# 각 batch 확인\n",
    "for d in ds.batch(3, drop_remainder=True):\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6,7이 batch로 출력되어야 하지만 drop_remainder=True 옵션 때문에 6,7을 drop하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.range(8)\n",
    "\n",
    "# 각 batch 확인\n",
    "for d in ds.batch(5, drop_remainder=True):\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "size가 5가 되지 않는 window는 drop되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. window : Time Series 데이터셋 생성에 유용\n",
    "\n",
    "인자\n",
    "\n",
    "+ window : 그룹화 할 윈도우 크기(갯수)\n",
    "+ drop_remainder : 남은 부분을 drop할 것인지 여부\n",
    "+ shift : 1번 반복할때마다 몇 개씩 이동할 것인지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[1, 2, 3, 4, 5]\n",
      "[2, 3, 4, 5, 6]\n",
      "[3, 4, 5, 6, 7]\n",
      "[4, 5, 6, 7, 8]\n",
      "[5, 6, 7, 8, 9]\n",
      "[6, 7, 8, 9]\n",
      "[7, 8, 9]\n",
      "[8, 9]\n",
      "[9]\n"
     ]
    }
   ],
   "source": [
    "# window의 크기는 5, sift=1, drop_remainder=False인 경우\n",
    "\n",
    "ds = tf.data.Dataset.range(10)\n",
    "ds = ds.window(5, shift=1, drop_remainder=False)\n",
    "for d in ds:\n",
    "    print(list(d.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[1, 2, 3, 4, 5]\n",
      "[2, 3, 4, 5, 6]\n",
      "[3, 4, 5, 6, 7]\n",
      "[4, 5, 6, 7, 8]\n",
      "[5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# window의 크기는 5, sift=1, drop_remainder=True인 경우\n",
    "\n",
    "ds = tf.data.Dataset.range(10)\n",
    "ds = ds.window(5, shift=1, drop_remainder=True)\n",
    "for d in ds:\n",
    "    print(list(d.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. flat_map : dataset에 함수를 apply해주고, 결과를 faltten하게 펼쳐준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2], shape=(3,), dtype=int64)\n",
      "tf.Tensor([3 4], shape=(2,), dtype=int64)\n",
      "tf.Tensor([1 2 3], shape=(3,), dtype=int64)\n",
      "tf.Tensor([4 5], shape=(2,), dtype=int64)\n",
      "tf.Tensor([2 3 4], shape=(3,), dtype=int64)\n",
      "tf.Tensor([5 6], shape=(2,), dtype=int64)\n",
      "tf.Tensor([3 4 5], shape=(3,), dtype=int64)\n",
      "tf.Tensor([6 7], shape=(2,), dtype=int64)\n",
      "tf.Tensor([4 5 6], shape=(3,), dtype=int64)\n",
      "tf.Tensor([7 8], shape=(2,), dtype=int64)\n",
      "tf.Tensor([5 6 7], shape=(3,), dtype=int64)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# lambda 함수를 통해 3개의 batch를 읽어들인 뒤 flatten된 리턴값을 받는다.\n",
    "\n",
    "ds = tf.data.Dataset.range(10)\n",
    "ds = ds.window(5, shift=1, drop_remainder=True)\n",
    "ds = ds.flat_map(lambda w: w.batch(3))\n",
    "for d in ds:\n",
    "    print(d) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int64)\n",
      "tf.Tensor([1 2 3 4 5], shape=(5,), dtype=int64)\n",
      "tf.Tensor([2 3 4 5 6], shape=(5,), dtype=int64)\n",
      "tf.Tensor([3 4 5 6 7], shape=(5,), dtype=int64)\n",
      "tf.Tensor([4 5 6 7 8], shape=(5,), dtype=int64)\n",
      "tf.Tensor([5 6 7 8 9], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# lambda 함수를 통해 5개의 batch를 읽어들인 뒤 flatten된 리턴값을 받는다.\n",
    "\n",
    "ds = tf.data.Dataset.range(10)\n",
    "ds = ds.window(5, shift=1, drop_remainder=True)\n",
    "ds = ds.flat_map(lambda w: w.batch(5))\n",
    "for d in ds:\n",
    "    print(d) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. shuffle \n",
    "\n",
    "suffle은 Dataset을 섞어주는 역할을 하며, 반드시 학습 전에 shuffle을 통해 적절하게 Dataset을 섞어주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 셔플을 해주지 않은 경우\n",
    "import numpy as np\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices(np.arange(10))\n",
    "for d in ds:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices(np.arange(10)).shuffle(buffer_size=5)\n",
    "\n",
    "for d in ds:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ shuffle의 인자 buffer_size\n",
    "\n",
    "데이터세트는 beffer_size 요소로 버퍼를 채운 다음에 버퍼에서 요소를 무작위로 샘플링하여 선택한 요소를 새 요소로 바꾼다. 완벽한 셔플링을 위해서는 데이터 세트의 전체 크기보다 크거나 같은 버퍼크기가 필요하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시\n",
    "dataset = dataset.shuffle(buffer_size=1000) # 1000이 옳은가?\n",
    "dataset = dataset.batch(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위코드를 실행하면 전체 데이터의 처음 1000개의 데이터를 buffer에 가져올것이고 그 다음에 1000개 중에 랜덤하게 100개를 pick할것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. map\n",
    "\n",
    "Dataset 전체에 함수를 매핑한다. Time Series Dataset을 만드려는 경우, train/label값을 분류하는 용도로 활용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set: [1 2 3 4]\n",
      "label set: [5]\n",
      "train set: [0 1 2 3]\n",
      "label set: [4]\n",
      "train set: [2 3 4 5]\n",
      "label set: [6]\n",
      "train set: [4 5 6 7]\n",
      "label set: [8]\n",
      "train set: [5 6 7 8]\n",
      "label set: [9]\n",
      "train set: [3 4 5 6]\n",
      "label set: [7]\n"
     ]
    }
   ],
   "source": [
    "window_size=5\n",
    "ds = tf.data.Dataset.range(10)\n",
    "ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
    "ds = ds.flat_map(lambda w: w.batch(window_size))\n",
    "ds = ds.shuffle(10)\n",
    "\n",
    "ds = ds.map(lambda x: (x[:-1], x[-1:]))\n",
    "for x, y in ds:\n",
    "    print('train set: {}'.format(x))\n",
    "    print('label set: {}'.format(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(케라스 1convid layer 번역 (https://keras.io/api/layers/convolution_layers/convolution1d/))\n",
    "\n",
    "+ filters : 정수, 출력 공간의 차원(즉, 컨볼루션의 출력 필터 수).\n",
    "\n",
    "+ kernel_size : 1D 컨볼루션 창의 길이를 지정하는 정수 또는 단일 정수의 튜플/목록.\n",
    "\n",
    "+ padding : \n",
    "     1. \"valid\" : 패딩 없음을 의미\n",
    "     2. \"same\" : 결과적으로 출력이 입력과 동일한 높이/너비 치수를 갖도록 입력의 왼쪽/오른쪽 또는 위/아래에 균일하게 0이 채워진다. \n",
    "     3. \"causal\" : 인과 관계(확장된) 컨볼루션이 발생합니다(예: output[t] 에 의존하지 않음 ) input[t+1:]. 모델이 시간 순서를 위반하지 않아야 하는 시간 데이터를 모델링할 때 유용하다.\n",
    "\n",
    "\n",
    "+ activation : 사용할 활성화 함수이다. 아무것도 지정하지 않으면 활성화 함수가 사용되지 않는다. (https://keras.io/ko/activations/ 참조)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model compile()\n",
    "\n",
    "complie() 메소드는 다음 세개의 인자를 입력으로 받는다.\n",
    "+ 정규화기 (optimizaer)\n",
    "  + 훈련과정을 설정한다. 즉, 최적화 알고리즘 설정을 의미한다.\n",
    "  + adam, sgd, rmsprop, adagrad 등이 잇다.\n",
    "+ 손실 함수 (loss function)\n",
    "  + 모델이 최적화에 사용되는 목적 함수이다.\n",
    "  + mse, categorical_crossentropy, binary_crossentropy 등이 있다.\n",
    "+ 평가지표 (metric)\n",
    "  + 훈련을 모니터링 하기 위해 사용된다.\n",
    "  + 분류에서는 accuracy, 회귀에서는 mse, rmse, r2, mae, mspe, mape, msle 등이 있다.\n",
    "  + 사용자가 메트릭을 정의해서 사용할 수도 있다. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2818b9f3110c02aba73766d4bb413b0d35712b55bde255418668dc206952d9ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
